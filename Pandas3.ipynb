{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTJZQe5VOEBCiqw8Yrruz+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dareoyeleke/python_scripting/blob/main/Pandas3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§® Pandas 3: Data Cleaning, Transformation, and Analytical Preparation\n",
        "\n",
        "This notebook focuses on **practical data cleaning and transformation workflows using Pandas**, with an emphasis on preparing datasets for analysis and visualization.\n",
        "\n",
        "Building on foundational Pandas concepts, this notebook demonstrates how to:\n",
        "- Manipulate and validate indexes and columns for consistency\n",
        "- Handle missing data using `.fillna()` and `.dropna()` based on analytical intent\n",
        "- Identify and remove duplicate records to maintain data integrity\n",
        "- Use `.copy()` safely to avoid chained assignment issues\n",
        "- Reshape data with `pivot_table()` to support aggregated analysis\n",
        "- Prepare cleaned datasets for plotting and exploratory analysis\n",
        "\n",
        "The goal of this notebook is not just to apply Pandas methods, but to **model real-world data preparation decisions** while analyzing data before visualization or downstream analysis.\n",
        "\n",
        "This workflow reflects common preprocessing tasks encountered in analytics, reporting, and business intelligence pipelines.\n",
        "\n",
        "\n",
        "## ðŸ› ï¸ Skills Demonstrated\n",
        "\n",
        "**Data Cleaning & Preparation**\n",
        "- Handling missing values with `.fillna()` and `.dropna()`\n",
        "- Identifying and removing duplicate records using `.drop_duplicates()`\n",
        "- Safely copying DataFrames with `.copy()` to avoid chained assignment issues\n",
        "\n",
        "**Data Manipulation & Structuring**\n",
        "- Index and column manipulation for consistency and readability\n",
        "- Reshaping datasets with `pivot_table()` for aggregated analysis\n",
        "- Applying transformations to prepare data for downstream use\n",
        "\n",
        "**Analytical Readiness**\n",
        "- Preparing cleaned datasets for visualization and exploratory analysis\n",
        "- Structuring data to support trend analysis and summary reporting\n",
        "- Applying real-world preprocessing decisions aligned with analytics workflows\n",
        "\n",
        "**Tools & Libraries**\n",
        "- Python\n",
        "- Pandas\n",
        "- Matplotlib (for post-cleaning visualization preparation)\n"
      ],
      "metadata": {
        "id": "sXxzjWTsmQBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dH3aQDksCGfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72015db7-6272-45e7-b3d5-4356ac2f8cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data for use\n",
        "dataset = load_dataset('lukebarousse/data_jobs')\n",
        "df = dataset['train'].to_pandas()\n",
        "\n",
        "# Little bit of data clean up\n",
        "df['job_posted_date'] = pd.to_datetime(df['job_posted_date'])\n",
        "\n",
        "# https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf. One stop shop cheat sheet as a reference for common Pandas information"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  I'll be showcasing merging DataFrames using a previously created pivot table showing the Number of Job Postings per month for every job position\n",
        "Once I get that done in this cell, I will be importing the DataFrame to be merged and then the real fun begins\n",
        "'''\n",
        "df_usa_jobs = df[df['job_country'] == \"United States\"]\n",
        "\n",
        "df_usa_jobs['job_month_posted'] = df_usa_jobs['job_posted_date'].dt.strftime('%B')\n",
        "# using the strftime function, we can print out the month from the job_posted_date from the df_usa_jobs DataFrame in a new column as the month names\n",
        "df_usa_jobs # Here we can check to confirm we have a column to prove the months were printed\n",
        "\n",
        "df_usa_month_pivot = df_usa_jobs.pivot_table(index='job_month_posted', columns='job_title_short', aggfunc='size')\n",
        "\n",
        "'''\n",
        "  This gives the number of job postings per month for every postion, taking NaN values also into consideration using 'size' as opposed to 'count' for the .aggfunc,\n",
        "however it prints out the months in alphabetical order as oppossed to chronologically, sorting can potentially change it to ascending or descending, but not chronological\n",
        "'''\n",
        "df_usa_month_pivot.reset_index(inplace=True) # We start by creating an index of numbers to attach the months to.\n",
        "\n",
        "df_usa_month_pivot['month_no'] = pd.to_datetime(df_usa_month_pivot['job_month_posted'], format='%B').dt.month  # creating a column aligning the month names with month numbers\n",
        "\n",
        "df_usa_month_pivot.sort_values('month_no', inplace=True) # sorting the month numbers, and thereby names in chronological order\n",
        "\n",
        "df_usa_month_pivot.set_index('job_month_posted', inplace=True) # and then setting the month numbers, month_no as the index.\n",
        "\n",
        "df_usa_month_pivot.drop(columns='month_no', inplace=True)\n",
        "# df_usa_month_pivot.drop(columns=['level_0', 'index', inplace=True]) used to drop recurring columns specified in list, columns\n",
        "\n",
        "df_usa_month_pivot # now we have our final table cleaned up for plotting\n",
        "\n",
        "data_jobs = ['Data Analyst', 'Data Engineer', 'Data Scientist'] # To narrow down to only the Data jobs I'm concerned about\n",
        "\n",
        "Data_job_month_demand = df_usa_month_pivot[data_jobs]\n",
        "\n",
        "Data_job_month_demand # However in the next cell I will be working with all the jobs in the table, not just Data jobs, so I will be using the DataFrame before filtering for Data Jobs, i.e df_usa_month_pivot\n",
        "\n",
        "\n",
        "df_usa_month_pivot # This is the DataFrame I will be merging the Software DataFrame with, how\n"
      ],
      "metadata": {
        "id": "qNicNW_CXc7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "software_jobs = pd.read_csv(\"https://lukeb.co/software_csv\") # Importing and creating a DataFrame for software jobs from the internet\n",
        "\n",
        "software_jobs = software_jobs.set_index('job_posted_month')  # Here we set the index to The month column\n",
        "\n",
        "software_jobs_DF = software_jobs.copy() # Create a copy so the original remains intact\n",
        "\n",
        "software_jobs_DF\n",
        "\n",
        "software_jobs_DF.rename(columns={'Front-End Developer' : 'Front_End_Dev', 'Back-End Developer' : 'Back_End_Dev', 'Full-Stack Developer' : 'Full_Stack_Dev', 'UI/UX Designer' : 'UI/UX_Des' })\n",
        "# rename the columns to make them easier to call\n",
        "\n",
        "# However to make merging easier, I'll make sure the identical columns have identical column names\n",
        "df_usa_month_pivot.index.name = 'job_posted_month'\n",
        "df_usa_month_pivot.index.name\n",
        "# since it is the index, I have to use the .index.name as opposed to .rename(column) to change the name, alternatively I could use left_on, and right_on to merge columns with different names\n",
        "\n",
        "# Now to merge both DataFrames\n",
        "\n",
        "df_US_merged_jobs = df_usa_month_pivot.merge(software_jobs_DF, on='job_posted_month')"
      ],
      "metadata": {
        "id": "RUIkRAq5DGRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  I will be creating another visual showing the top 5 Jobs from the merged Table to see the Trends in Jobs over the course of the year.\n",
        "To get the top 5 jobs, we'll be doing a sum of the job counts to see the top 5 first, and then plotting those 5 jobs\n",
        "'''\n",
        "\n",
        "top_5_jobs = (df_US_merged_jobs\n",
        "              .sum() # sum of all job positions\n",
        "              .sort_values(ascending=False) # sort the sums from ascending to descending\n",
        "              .head() # gives the top 5 values\n",
        "              .index) # prints out the index values\n",
        "\n",
        "top_5_jobs # The names of the top 5 jobs we need for the visual\n",
        "\n",
        "df_US_merged_jobs[top_5_jobs].plot() # plots a line chart by default\n",
        "plt.title('Job Postings Per Month for Top Tech Jobs for 2023')\n",
        "plt.ylabel('No of Job Postings')\n",
        "plt.xlabel('Year 2023')\n",
        "plt.xticks(rotation= 45)\n",
        "plt.legend(title='Job Title')\n",
        "plt.ylim(0, 25000)\n",
        "plt.show()\n",
        "\n",
        "# We now have a line chart showing the Trends in Job Postings for year 2023\n"
      ],
      "metadata": {
        "id": "Gl4KjMr0aCD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Next using Concatenate method, I will be creating a DataFrame, to represent the Data for the first yearly quater, i.e Jan - March.\n",
        "Concatenate, as opposed to Merge( adding more columns to the DataFrames), Bascially adds more rows. I.e Merge is sideways increasing,\n",
        "concatenate is lengthwise increasing. To do that, I'll be creating some DataFrames by segmenting data from DataFrame(df) by the month and then concatenating them.\n",
        "'''\n",
        "df['job_posted_month'] = df['job_posted_date'].dt.strftime('%b') # first i create a new column pulling out shortened month names from the date/time column\n",
        "\n",
        "months = df['job_posted_month'].unique() # and then put those values in a list and store them in an object\n",
        "\n",
        "months\n",
        "\n",
        "dict_months = {month : df[df['job_posted_month'] == month] for month in months }\n",
        "'''\n",
        "  I use dict comprehension for every value in the object month printed out with the key as the month, and the value as a DataFrame printed out,\n",
        "filtering the DataFrame(df) for the month called through every iteration of the loop. i.e if the month is Jan, filter df where the\n",
        "'job_posted_month' is also Jan and store the key value as Jan, repeat this for every month value in months(holds all the month values)\n",
        "'''\n",
        "\n",
        "dict_months['Jan'] # verifying that it works by checking for the month of Jan\n",
        "\n",
        "# Now to finally concat the data\n",
        "df_Q1 = pd.concat([dict_months['Jan'], dict_months['Feb'], dict_months['Mar']], ignore_index=True) # Here we have the Job Postings for Q1 in order from Jan to March\n",
        "df_Q1"
      ],
      "metadata": {
        "id": "lZG-BcwDhXbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Exporting Data to different formats using the .to_() to other data formats like csv, Excel and even sql if needed\n",
        "'''\n",
        "df_Q1.to_csv(' Quater_1.csv', index=False)\n",
        "#saved as a csv file, with the .csv extension allows it to properly format as a csv file, keeping the index False allows the csv file to create its own index, preventing multiple indxes from automatically existing.\n",
        "\n",
        "import openpyxl\n",
        "\n",
        "df_Q1.to_excel('Quarter_1.xlsx') # Exported to excel as a .xlsx file\n",
        "\n"
      ],
      "metadata": {
        "id": "WrtV94rI1Hgi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XlRLfcxIjb3e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}